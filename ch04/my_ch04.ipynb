{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Chapter 4. 의사결정나무로 산림 식생 분포 예측하기__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ~ 4.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지도학습(Supervised Learning) ⊃ 분류, 회귀  \n",
    "* 분류/회귀 모두에 적용 가능한, 대중적이고 유연한 알고리즘 __\"Decision Tree\"__, Decision Tree의 확장판인 __\"RandomForest\"__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특징(Feature) = 차원(Dimension) = 예측변수(Predictor) = 변수(Variable)\n",
    "* Categorical Feature , Numeric Feature      \n",
    "* 목표(Target) : 회귀에서는 수를, 분류에서는 범주를 대상으로 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 의사결정나무와 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Decision Tree\n",
    "* 범주형, 수치형 변수 모두 다룰 수 있다.  \n",
    "* 단일 트리와 다중 트리 모두 병렬로 구축할 수 있다.   \n",
    "* 이상치에 잘 휘둘리지 않는다 (극단적인 오류값들이 예측치에 영향을 줄 수 없다)  \n",
    "* 전처리나 정규화를 거치지 않고도 다른 유형과 다른 척도의 데이터를 다룰 수 있다.  \n",
    "  \n",
    "(2) Random Forest  \n",
    "* Decision Tree를 더 강력하게 일반화한 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree기반 알고리즘의 장점 : 직관적으로 이해 및 추론할 수 있다. (일련의 예/아니오 선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Covtype 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gzip -d covtype.data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "datapath = './'\n",
    "DRIVER_MEMORY = \"8g\"\n",
    "\n",
    "spark = SparkSession.builder.appName('ch04')\\\n",
    "    .master('local[*]')\\\n",
    "    .config('spark.sql.warehouse.dir', datapath)\\\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "dataWithoutHeader = spark.read\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .option('header', False)\\\n",
    "    .csv('covtype.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 55)\n"
     ]
    }
   ],
   "source": [
    "print((dataWithoutHeader.count(), len(dataWithoutHeader.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 변수명 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colinfo = \"\"\"\n",
    "Elevation                               quantitative    meters                       Elevation in meters\n",
    "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "Slope                                   quantitative    degrees                      Slope in degrees\n",
    "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "Cover_Type (7 types)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elevation',\n",
       " 'Aspect',\n",
       " 'Slope',\n",
       " 'Horizontal_Distance_To_Hydrology',\n",
       " 'Vertical_Distance_To_Hydrology',\n",
       " 'Horizontal_Distance_To_Roadways',\n",
       " 'Hillshade_9am',\n",
       " 'Hillshade_Noon',\n",
       " 'Hillshade_3pm',\n",
       " 'Horizontal_Distance_To_Fire_Points',\n",
       " 'Wilderness_Area',\n",
       " 'Soil_Type',\n",
       " 'Cover_Type']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = []\n",
    "for line in colinfo.split('\\n') :\n",
    "    if len(line) > 0 :\n",
    "        a_colname = line.split(' ')[0].strip()\n",
    "        colNames.append(a_colname)\n",
    "colNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Soil_Type_36',\n",
       " 'Soil_Type_37',\n",
       " 'Soil_Type_38',\n",
       " 'Soil_Type_39',\n",
       " 'Soil_Type_40']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding column name\n",
    "ohe_colNames = []\n",
    "for a_ohe_colname, n_class in [('Wilderness_Area', 4), ('Soil_Type', 40)] :\n",
    "    for n in range(n_class) :\n",
    "        ohe_colNames.append(a_ohe_colname + '_' + str(n+1))\n",
    "\n",
    "print(len(ohe_colNames))\n",
    "ohe_colNames[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Elevation',\n",
       " 'Aspect',\n",
       " 'Slope',\n",
       " 'Horizontal_Distance_To_Hydrology',\n",
       " 'Vertical_Distance_To_Hydrology',\n",
       " 'Horizontal_Distance_To_Roadways',\n",
       " 'Hillshade_9am',\n",
       " 'Hillshade_Noon',\n",
       " 'Hillshade_3pm',\n",
       " 'Horizontal_Distance_To_Fire_Points',\n",
       " 'Wilderness_Area_1',\n",
       " 'Wilderness_Area_2',\n",
       " 'Wilderness_Area_3',\n",
       " 'Wilderness_Area_4',\n",
       " 'Soil_Type_1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = colNames[:-3] + ohe_colNames + [colNames[-1]]\n",
    "print(len(colNames))\n",
    "colNames[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataWithoutHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation: int, Aspect: int, Slope: int, Horizontal_Distance_To_Hydrology: int, Vertical_Distance_To_Hydrology: int, Horizontal_Distance_To_Roadways: int, Hillshade_9am: int, Hillshade_Noon: int, Hillshade_3pm: int, Horizontal_Distance_To_Fire_Points: int, Wilderness_Area_1: int, Wilderness_Area_2: int, Wilderness_Area_3: int, Wilderness_Area_4: int, Soil_Type_1: int, Soil_Type_2: int, Soil_Type_3: int, Soil_Type_4: int, Soil_Type_5: int, Soil_Type_6: int, Soil_Type_7: int, Soil_Type_8: int, Soil_Type_9: int, Soil_Type_10: int, Soil_Type_11: int, Soil_Type_12: int, Soil_Type_13: int, Soil_Type_14: int, Soil_Type_15: int, Soil_Type_16: int, Soil_Type_17: int, Soil_Type_18: int, Soil_Type_19: int, Soil_Type_20: int, Soil_Type_21: int, Soil_Type_22: int, Soil_Type_23: int, Soil_Type_24: int, Soil_Type_25: int, Soil_Type_26: int, Soil_Type_27: int, Soil_Type_28: int, Soil_Type_29: int, Soil_Type_30: int, Soil_Type_31: int, Soil_Type_32: int, Soil_Type_33: int, Soil_Type_34: int, Soil_Type_35: int, Soil_Type_36: int, Soil_Type_37: int, Soil_Type_38: int, Soil_Type_39: int, Soil_Type_40: int, Cover_Type: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataWithoutHeader.toDF(*colNames).withColumn('Cover_Type', F.col('Cover_Type').cast(\"double\"))\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Cover_Type| count|\n",
      "+----------+------+\n",
      "|       2.0|283301|\n",
      "|       1.0|211840|\n",
      "|       3.0| 35754|\n",
      "|       7.0| 20510|\n",
      "|       6.0| 17367|\n",
      "|       5.0|  9493|\n",
      "|       4.0|  2747|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_distribution = data.groupBy('Cover_Type').count().orderBy('count', ascending=False)\n",
    "target_distribution.show()   # imbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cover_Type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.487599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.061537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.029891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.016339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.004728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cover_Type     count\n",
       "0         2.0  0.487599\n",
       "1         1.0  0.364605\n",
       "2         3.0  0.061537\n",
       "3         7.0  0.035300\n",
       "4         6.0  0.029891\n",
       "5         5.0  0.016339\n",
       "6         4.0  0.004728"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dist = target_distribution.withColumn('count', F.col('count') / data.count()).toPandas()  # Type2가 약 50%, Type2 & Type1이 약 85%.\n",
    "y_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. 첫 번째 의사 결정 나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train(+Validation) : Test = 9 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData = data.randomSplit([0.9, 0.1], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation: int, Aspect: int, Slope: int, Horizontal_Distance_To_Hydrology: int, Vertical_Distance_To_Hydrology: int, Horizontal_Distance_To_Roadways: int, Hillshade_9am: int, Hillshade_Noon: int, Hillshade_3pm: int, Horizontal_Distance_To_Fire_Points: int, Wilderness_Area_1: int, Wilderness_Area_2: int, Wilderness_Area_3: int, Wilderness_Area_4: int, Soil_Type_1: int, Soil_Type_2: int, Soil_Type_3: int, Soil_Type_4: int, Soil_Type_5: int, Soil_Type_6: int, Soil_Type_7: int, Soil_Type_8: int, Soil_Type_9: int, Soil_Type_10: int, Soil_Type_11: int, Soil_Type_12: int, Soil_Type_13: int, Soil_Type_14: int, Soil_Type_15: int, Soil_Type_16: int, Soil_Type_17: int, Soil_Type_18: int, Soil_Type_19: int, Soil_Type_20: int, Soil_Type_21: int, Soil_Type_22: int, Soil_Type_23: int, Soil_Type_24: int, Soil_Type_25: int, Soil_Type_26: int, Soil_Type_27: int, Soil_Type_28: int, Soil_Type_29: int, Soil_Type_30: int, Soil_Type_31: int, Soil_Type_32: int, Soil_Type_33: int, Soil_Type_34: int, Soil_Type_35: int, Soil_Type_36: int, Soil_Type_37: int, Soil_Type_38: int, Soil_Type_39: int, Soil_Type_40: int, Cover_Type: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cover_Type</th>\n",
       "      <th>count</th>\n",
       "      <th>Cover_Type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.487610</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.487503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.061725</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.059837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.035257</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.035691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.029967</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.017136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.004691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cover_Type     count  Cover_Type     count\n",
       "4         2.0  0.487610         2.0  0.487503\n",
       "1         1.0  0.364457         1.0  0.365943\n",
       "3         3.0  0.061725         3.0  0.059837\n",
       "0         7.0  0.035257         7.0  0.035691\n",
       "5         6.0  0.029967         6.0  0.029200\n",
       "6         5.0  0.016251         5.0  0.017136\n",
       "2         4.0  0.004732         4.0  0.004691"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (수정사항)   Y distribution 그대로 Sampling 됐을까?   Yes :)\n",
    "import pandas as pd\n",
    "dist_train1 = trainData.groupBy('Cover_Type').count().withColumn('count', F.col('count') / trainData.count()).toPandas().sort_values(by='count', ascending=False) \n",
    "dist_test1  = testData.groupBy('Cover_Type').count().withColumn('count', F.col('count') / testData.count()).toPandas().sort_values(by='count', ascending=False)\n",
    "pd.concat([dist_train1, dist_test1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLlib는 feature들을 모아 하나의 열로 구성한 벡터를 사용한다 --> VectorAssembler 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0]) |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputCols = [c for c in trainData.columns if c != 'Cover_Type']\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(inputCols)\\\n",
    "    .setOutputCol(\"featureVector\")  # 결과로 얻은 DF에서는 새로운 'featureVector' 열이 만들어진다.\n",
    "assembledTrainData = assembler.transform(trainData)\n",
    "\n",
    "assembledTrainData.select('featureVector').show(n=5, truncate=False)  \n",
    "# 저장공간을 절약하기 위해 SparseVector 인스턴스로 표현.\n",
    "# 54개 값 대부분이 0 이기 때문에, 0이 아닌 경우의 인덱스와 값만 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DecisionTree 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# from pyspark.util import Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_06ab10692562, depth=5, numNodes=43, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3045.5)\n",
      "   If (feature 0 <= 2563.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 0 <= 2466.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       Predict: 4.0\n",
      "      Else (feature 3 > 15.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2466.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     Predict: 2.0\n",
      "   Else (feature 0 > 2563.5)\n",
      "    If (feature 0 <= 2952.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 0 > 2952.5)\n",
      "     If (feature 3 <= 211.0)\n",
      "      If (feature 36 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 36 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 211.0)\n",
      "      Predict: 2.0\n",
      "  Else (feature 0 > 3045.5)\n",
      "   If (feature 0 <= 3333.5)\n",
      "    If (feature 7 <= 239.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 239.5)\n",
      "     If (feature 3 <= 321.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 321.0)\n",
      "      If (feature 0 <= 3216.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3216.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3333.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 4 <= 52.5)\n",
      "      If (feature 7 <= 238.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 7 > 238.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 4 > 52.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      If (feature 9 <= 457.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 9 > 457.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 926.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 926.5)\n",
      "       Predict: 1.0\n",
      "\n",
      "CPU times: user 32.3 ms, sys: 443 µs, total: 32.7 ms\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = DecisionTreeClassifier()\\\n",
    "    .setSeed(123)\\\n",
    "    .setLabelCol('Cover_Type')\\\n",
    "    .setFeaturesCol('featureVector')\\\n",
    "    .setPredictionCol('prediction')     # 예측값을 저장할 column name\n",
    "\n",
    "model = classifier.fit(assembledTrainData)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(54, {0: 0.8035, 3: 0.0384, 4: 0.0034, 5: 0.0019, 7: 0.0269, 9: 0.0034, 10: 0.0326, 12: 0.0113, 15: 0.0236, 17: 0.0304, 36: 0.0064, 45: 0.0183})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80352448, 0.        , 0.        , 0.03835108, 0.00338292,\n",
       "       0.00186985, 0.        , 0.02689828, 0.        , 0.00340973,\n",
       "       0.03260615, 0.        , 0.01130231, 0.        , 0.        ,\n",
       "       0.02360167, 0.        , 0.03039992, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00635212, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01830148, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_imp = model.featureImportances.toArray()\n",
    "type(ft_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elevation</td>\n",
       "      <td>0.803524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Horizontal_Distance_To_Hydrology</td>\n",
       "      <td>0.038351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wilderness_Area_1</td>\n",
       "      <td>0.032606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Soil_Type_4</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hillshade_Noon</td>\n",
       "      <td>0.026898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Soil_Type_2</td>\n",
       "      <td>0.023602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Soil_Type_32</td>\n",
       "      <td>0.018301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wilderness_Area_3</td>\n",
       "      <td>0.011302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Soil_Type_23</td>\n",
       "      <td>0.006352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Horizontal_Distance_To_Fire_Points</td>\n",
       "      <td>0.003410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vertical_Distance_To_Hydrology</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Horizontal_Distance_To_Roadways</td>\n",
       "      <td>0.001870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Soil_Type_29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Soil_Type_21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Soil_Type_22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           column_name  importances\n",
       "0                            Elevation     0.803524\n",
       "3     Horizontal_Distance_To_Hydrology     0.038351\n",
       "10                   Wilderness_Area_1     0.032606\n",
       "17                         Soil_Type_4     0.030400\n",
       "7                       Hillshade_Noon     0.026898\n",
       "15                         Soil_Type_2     0.023602\n",
       "45                        Soil_Type_32     0.018301\n",
       "12                   Wilderness_Area_3     0.011302\n",
       "36                        Soil_Type_23     0.006352\n",
       "9   Horizontal_Distance_To_Fire_Points     0.003410\n",
       "4       Vertical_Distance_To_Hydrology     0.003383\n",
       "5      Horizontal_Distance_To_Roadways     0.001870\n",
       "42                        Soil_Type_29     0.000000\n",
       "34                        Soil_Type_21     0.000000\n",
       "35                        Soil_Type_22     0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_imp = pd.DataFrame(zip(inputCols, ft_imp), columns=['column_name', 'importances'])\n",
    "df_imp.sort_values(by='importances', ascending=False).head(15)  # 영향을 주는 변수는 12개 뿐이네..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습데이터로 예측해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                       |\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,0.0,0.03994933255383416,0.6242164409366981,0.04777680340381305,0.0,0.28805742310565463,0.0]                  |\n",
      "|6.0       |4.0       |[0.0,4.1631973355537054E-4,0.05328892589508743,0.28184845961698585,0.4121565362198168,0.0,0.25228975853455454,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.03994933255383416,0.6242164409366981,0.04777680340381305,0.0,0.28805742310565463,0.0]                  |\n",
      "|6.0       |3.0       |[0.0,0.0,0.03994933255383416,0.6242164409366981,0.04777680340381305,0.0,0.28805742310565463,0.0]                  |\n",
      "|6.0       |3.0       |[0.0,0.0,0.03994933255383416,0.6242164409366981,0.04777680340381305,0.0,0.28805742310565463,0.0]                  |\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(assembledTrainData)\n",
    "predictions.select('Cover_Type', 'prediction', 'probability').show(n = 5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target class는 7개 뿐이지만, probability 벡터 값은 8개다.  \n",
    "맨 앞은 항상 0.0이며, 무의미한 내용이다. (정보를 벡터로 표현할 때의 습관과 같은 것이며 조심해야한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7026,  F1-Score : 0.6854\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "    .setLabelCol('Cover_Type')\\\n",
    "    .setPredictionCol('prediction')\n",
    "\n",
    "acc = evaluator.setMetricName('accuracy').evaluate(predictions)\n",
    "f1 = evaluator.setMetricName('f1').evaluate(predictions)\n",
    "print('Accuracy : {:.4f},  F1-Score : {:.4f}'.format(acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionRDD = predictions\\\n",
    "    .withColumn('prediction', F.col('prediction').cast('double'))\\\n",
    "    .withColumn('Cover_Type', F.col('Cover_Type').cast('double'))\\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523238"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Elevation=1863, Aspect=37, Slope=17, Horizontal_Distance_To_Hydrology=120, Vertical_Distance_To_Hydrology=18, Horizontal_Distance_To_Roadways=90, Hillshade_9am=217, Hillshade_Noon=202, Hillshade_3pm=115, Horizontal_Distance_To_Fire_Points=769, Wilderness_Area_1=0, Wilderness_Area_2=0, Wilderness_Area_3=0, Wilderness_Area_4=1, Soil_Type_1=0, Soil_Type_2=1, Soil_Type_3=0, Soil_Type_4=0, Soil_Type_5=0, Soil_Type_6=0, Soil_Type_7=0, Soil_Type_8=0, Soil_Type_9=0, Soil_Type_10=0, Soil_Type_11=0, Soil_Type_12=0, Soil_Type_13=0, Soil_Type_14=0, Soil_Type_15=0, Soil_Type_16=0, Soil_Type_17=0, Soil_Type_18=0, Soil_Type_19=0, Soil_Type_20=0, Soil_Type_21=0, Soil_Type_22=0, Soil_Type_23=0, Soil_Type_24=0, Soil_Type_25=0, Soil_Type_26=0, Soil_Type_27=0, Soil_Type_28=0, Soil_Type_29=0, Soil_Type_30=0, Soil_Type_31=0, Soil_Type_32=0, Soil_Type_33=0, Soil_Type_34=0, Soil_Type_35=0, Soil_Type_36=0, Soil_Type_37=0, Soil_Type_38=0, Soil_Type_39=0, Soil_Type_40=0, Cover_Type=6.0, featureVector=SparseVector(54, {0: 1863.0, 1: 37.0, 2: 17.0, 3: 120.0, 4: 18.0, 5: 90.0, 6: 217.0, 7: 202.0, 8: 115.0, 9: 769.0, 13: 1.0, 15: 1.0}), rawPrediction=DenseVector([0.0, 0.0, 1230.0, 19219.0, 1471.0, 0.0, 8869.0, 0.0]), probability=DenseVector([0.0, 0.0, 0.0399, 0.6242, 0.0478, 0.0, 0.2881, 0.0]), prediction=3.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassMetrics = MulticlassMetrics(predictionRDD)\n",
    "multiclassMetrics.confusionMatrix()   # ValueError: Length of object (59) does not match with length of fields (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+---+---+---+----+\n",
      "|Cover_Type|     1|     2|    3|  4|  5|  6|   7|\n",
      "+----------+------+------+-----+---+---+---+----+\n",
      "|       1.0|135581| 51194|  167|  1|  0|  0|3755|\n",
      "|       2.0| 54825|195195| 4526|128|  0|  0| 462|\n",
      "|       3.0|     0|  5213|26407|677|  0|  0|   0|\n",
      "|       4.0|     0|    15| 1471|990|  0|  0|   0|\n",
      "|       5.0|    14|  7802|  687|  0|  0|  0|   0|\n",
      "|       6.0|     0|  5528| 9546|606|  0|  0|   0|\n",
      "|       7.0|  8928|    22|   53|  0|  0|  0|9445|\n",
      "+----------+------+------+-----+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix = predictions\\\n",
    "    .groupBy('Cover_Type')\\\n",
    "    .pivot('prediction', [i+1 for i in range(7)])\\\n",
    "    .count()\\\n",
    "    .fillna(0)\\\n",
    "    .orderBy('Cover_Type')\n",
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train & Testset 예측성능 모두를 고려한 전체 정확도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classProba(df) :\n",
    "    total = df.count()\n",
    "    list_proba = df\\\n",
    "        .groupBy('Cover_Type')\\\n",
    "        .count()\\\n",
    "        .orderBy('Cover_Type')\\\n",
    "        .select(F.col('count').cast('double'))\\\n",
    "        .rdd.map(lambda x: x['count'] / total)\\\n",
    "        .collect()\n",
    "    return list_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPriorProba = classProba(trainData)\n",
    "testPriorProba  = classProba(testData)\n",
    "accuracy = sum([x * y for x, y in zip(trainPriorProba, testPriorProba)])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. 의사결정나무 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* maxDepth : 결정의 최대 갯수. 과적합을 막으려면 depth를 제한하는 것이 좋다.   \n",
    "* maxBins  : 결정 규칙에 집어넣을 값들의 집합. bin이 크면 처리시간이 길어지지만, 더 최적화된 결정 규칙을 찾을 수 있다.   \n",
    "* impurity : 좋은 결정규칙을 판단하는 '불순도'의 측정 방법. 보통 gini(default)와 entropy를 사용하며, 둘다 값이 작을수록 좋다.  \n",
    "* minInfoGain : 최소정보획득량. 과적합 방지 목적. 부분집합의 불순도를 충분히 개선하지 못하는 규칙은 기각된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. 의사결정나무 튜닝하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 두 단계를 캡슐화하는 Pipeline 객체를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [c for c in trainData.columns if c != 'Cover_Type']\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(inputCols)\\\n",
    "    .setOutputCol(\"featureVector\")\n",
    "\n",
    "classifier = DecisionTreeClassifier()\\\n",
    "    .setSeed(123)\\\n",
    "    .setLabelCol('Cover_Type')\\\n",
    "    .setFeaturesCol('featureVector')\\\n",
    "    .setPredictionCol('prediction')\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하이퍼파라미터 조합 정의 - param당 2개씩을 후보로, 총 16개 모델을 구축하고 평가하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(classifier.impurity, ['gini', 'entropy'])\\\n",
    "    .addGrid(classifier.maxDepth, [1, 20])\\\n",
    "    .addGrid(classifier.maxBins, [40, 300])\\\n",
    "    .addGrid(classifier.minInfoGain, [0, 0.05])\\\n",
    "    .build()\n",
    "\n",
    "multiClassEval = MulticlassClassificationEvaluator()\\\n",
    "    .setLabelCol('Cover_Type')\\\n",
    "    .setPredictionCol('prediction')\\\n",
    "    .setMetricName('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TrainValidationSplit을 활용한 평가  \n",
    "(cf. k-Fold CrossValidator : 비용이 k배 더 들고, 빅데이터에서는 추가로 얻는 이득이 많지 않다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 1.11 s, total: 3.06 s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "validator = TrainValidationSplit()\\\n",
    "    .setSeed(123)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(multiClassEval)\\\n",
    "    .setEstimatorParamMaps(paramGrid)\\\n",
    "    .setTrainRatio(0.9)\n",
    "\n",
    "validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최적 모델의 하이퍼파라미터를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "bestModel = validatorModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_map = bestModel.stages[-1].extractParamMap()\n",
    "len(param_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Param(parent='DecisionTreeClassifier_ab3c8a9e8b49', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'), False) \n",
      "\n",
      "(Param(parent='DecisionTreeClassifier_ab3c8a9e8b49', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'), 10) \n",
      "\n",
      "(Param(parent='DecisionTreeClassifier_ab3c8a9e8b49', name='featuresCol', doc='features column name.'), 'featureVector') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, a_pair in enumerate(param_map.items()) :\n",
    "    if i >= 3 :\n",
    "        break\n",
    "    else :\n",
    "        print(a_pair, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cacheNodeIds': False,\n",
       " 'checkpointInterval': 10,\n",
       " 'featuresCol': 'featureVector',\n",
       " 'impurity': 'entropy',\n",
       " 'labelCol': 'Cover_Type',\n",
       " 'leafCol': '',\n",
       " 'maxBins': 40,\n",
       " 'maxDepth': 20,\n",
       " 'maxMemoryInMB': 256,\n",
       " 'minInfoGain': 0.0,\n",
       " 'minInstancesPerNode': 1,\n",
       " 'minWeightFractionPerNode': 0.0,\n",
       " 'predictionCol': 'prediction',\n",
       " 'probabilityCol': 'probability',\n",
       " 'rawPredictionCol': 'rawPrediction',\n",
       " 'seed': 123}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_param_map(param_map) :\n",
    "    rtn_dict = {}\n",
    "    for k,v in param_map.items() :\n",
    "        rtn_dict.update({str(k).split('__')[-1] : v})\n",
    "    return rtn_dict\n",
    "        \n",
    "print_param_map(param_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.31 s, sys: 1.24 s, total: 3.55 s\n",
      "Wall time: 5min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "validatorModel = validator.fit(trainData)\n",
    "\n",
    "temp_pnm = validatorModel.validationMetrics\n",
    "type(temp_pnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633552340031984"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_pnm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111963160632743\n",
      "{'impurity': 'entropy', 'maxDepth': 20, 'maxBins': 40, 'minInfoGain': 0.0}\n",
      "\n",
      "0.911061443902815\n",
      "{'impurity': 'entropy', 'maxDepth': 20, 'maxBins': 300, 'minInfoGain': 0.0}\n",
      "\n",
      "0.9044334405888134\n",
      "{'impurity': 'gini', 'maxDepth': 20, 'maxBins': 40, 'minInfoGain': 0.0}\n",
      "\n",
      "0.9031039864357141\n",
      "{'impurity': 'gini', 'maxDepth': 20, 'maxBins': 300, 'minInfoGain': 0.0}\n",
      "\n",
      "0.7263443864280071\n",
      "{'impurity': 'entropy', 'maxDepth': 20, 'maxBins': 300, 'minInfoGain': 0.05}\n",
      "\n",
      "0.7243405714726113\n",
      "{'impurity': 'entropy', 'maxDepth': 20, 'maxBins': 40, 'minInfoGain': 0.05}\n",
      "\n",
      "0.6692549276507196\n",
      "{'impurity': 'gini', 'maxDepth': 20, 'maxBins': 300, 'minInfoGain': 0.05}\n",
      "\n",
      "0.6690044507812951\n",
      "{'impurity': 'gini', 'maxDepth': 20, 'maxBins': 40, 'minInfoGain': 0.05}\n",
      "\n",
      "0.63376428199842\n",
      "{'impurity': 'gini', 'maxDepth': 1, 'maxBins': 300, 'minInfoGain': 0.0}\n",
      "\n",
      "0.63376428199842\n",
      "{'impurity': 'gini', 'maxDepth': 1, 'maxBins': 300, 'minInfoGain': 0.05}\n",
      "\n",
      "0.633552340031984\n",
      "{'impurity': 'gini', 'maxDepth': 1, 'maxBins': 40, 'minInfoGain': 0.0}\n",
      "\n",
      "0.633552340031984\n",
      "{'impurity': 'gini', 'maxDepth': 1, 'maxBins': 40, 'minInfoGain': 0.05}\n",
      "\n",
      "0.48918132598601183\n",
      "{'impurity': 'entropy', 'maxDepth': 1, 'maxBins': 40, 'minInfoGain': 0.0}\n",
      "\n",
      "0.48918132598601183\n",
      "{'impurity': 'entropy', 'maxDepth': 1, 'maxBins': 40, 'minInfoGain': 0.05}\n",
      "\n",
      "0.48918132598601183\n",
      "{'impurity': 'entropy', 'maxDepth': 1, 'maxBins': 300, 'minInfoGain': 0.0}\n",
      "\n",
      "0.48918132598601183\n",
      "{'impurity': 'entropy', 'maxDepth': 1, 'maxBins': 300, 'minInfoGain': 0.05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paramsAndMetrics = [(metric, param) for metric, param in zip(temp_pnm, validatorModel.getEstimatorParamMaps())]\n",
    "paramsAndMetrics = sorted(paramsAndMetrics, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "for metric, param in paramsAndMetrics :\n",
    "    print(metric)\n",
    "    print(print_param_map(param))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모델이 Test dataset 으로는 얼마의 정확도를 달성할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.914754041610413"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiClassEval.evaluate(bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. 범주형 특징 다시 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 40가지 값을 가진 1개의 범주형 변수를 40개의 수치형 변수로 표현하면, 메모리 사용량이 늘고 성능은 나빠진다.\n",
    "* One-hot Encoding을 사용하지 않으면 어떨까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas UDF  \n",
    "https://docs.microsoft.com/ko-kr/azure/databricks/spark/latest/spark-sql/udf-python-pandas   \n",
    "\n",
    "    - a user-defined function that uses Apache Arrow to transfer data and pandas to work with the data.\n",
    "    - You define a pandas UDF using the keyword \"pandas_udf\" as a __decorator__ and wrap the function with a __Python type hint__. \n",
    "    - (1) Series --> Series   \n",
    "    - (2) Iterator of Series --> Iterator of Series   \n",
    "    - (3) Iterator of multiple Series --> Iterator of Series  \n",
    "    - (4) Series --> scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wilderness_Area_1',\n",
       " 'Wilderness_Area_2',\n",
       " 'Wilderness_Area_3',\n",
       " 'Wilderness_Area_4']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilder_cols = [c for c in inputCols if 'Wilder' in c]\n",
    "wilder_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+---------------+\n",
      "|Wilderness_Area_1|Wilderness_Area_2|Wilderness_Area_3|Wilderness_Area_4|wilderness_area|\n",
      "+-----------------+-----------------+-----------------+-----------------+---------------+\n",
      "|                1|                0|                0|                0|   [1, 0, 0, 0]|\n",
      "|                1|                0|                0|                0|   [1, 0, 0, 0]|\n",
      "|                1|                0|                0|                0|   [1, 0, 0, 0]|\n",
      "+-----------------+-----------------+-----------------+-----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 일단 Array로 묶어서 하나의 컬럼으로 만들어본다.\n",
    "df_wilder = data.select(*wilder_cols, F.array(*wilder_cols).alias('wilderness_area'))\n",
    "df_wilder.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Wilderness_Area_4: integer (nullable = true)\n",
      " |-- wilderness_area: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wilder.printSchema()  # Array 라는 점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np\n",
    "\n",
    "@F.pandas_udf(T.LongType())\n",
    "def unhot_udf(arrs: pd.Series) -> pd.Series :\n",
    "    return pd.Series([np.argmax(a)+1 for a in arrs])  # Array에서 가장 큰값의 위치를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Wilderness_Area|\n",
      "+---------------+\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wilder.select(unhot_udf(F.col('wilderness_area')).alias('Wilderness_Area')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
